# app.py â€” StudyMind AI (Pastel-Purple Apple-style, Pro Word formatting)
# Date: 2025-11-13 (Asia/Seoul)
# Run: streamlit run app.py
# ğŸŒŸ v2.11: 3ê°€ì§€ ì‹ ê·œ ìš”ì²­ ì‚¬í•­ ë°˜ì˜ (v2.8 ê¸°ë°˜)
# (Word ë§ˆì¸ë“œë§µ 6.5x9.5 ë¹„ìœ¨ ìœ ì§€ ìŠ¤ì¼€ì¼ë§, Pillow ì˜ì¡´ì„± ì¶”ê°€)

import os
import io
import re
import json
import uuid
import textwrap
from typing import List, Dict, Tuple
from contextlib import contextmanager

import streamlit as st
from PyPDF2 import PdfReader

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Optional: graphviz for mindmap rendering â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import graphviz as gv
    GV_AVAILABLE = True
except Exception:
    GV_AVAILABLE = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒŸ (Req #1 v2.11) Pillow(PIL) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    st.error("Pillow ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. 'pip install Pillow'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    st.stop()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Word export (python-docx) â”€â”€â”€â”€â”€â”€â”€â”€â”€
from docx import Document
from docx.shared import Pt, Inches
from docx.oxml.ns import qn
from docx.enum.table import WD_TABLE_ALIGNMENT
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml import OxmlElement

TABLE_STYLE_NAME = "Table Grid"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Web search & scraping (fallback chain) â”€â”€â”€â”€â”€â”€â”€â”€â”€
import requests
try:
    from duckduckgo_search import DDGS
    DDG_AVAILABLE = True
except Exception:
    DDG_AVAILABLE = False

try:
    import trafilatura
    TRA_AVAILABLE = True
except Exception:
    TRA_AVAILABLE = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ OpenAI SDK (version-safe wrapper) â”€â”€â”€â”€â”€â”€â”€â”€â”€
from openai import OpenAI
from openai import AuthenticationError, RateLimitError, OpenAIError

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ensure run via Streamlit â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from streamlit.runtime.scriptrunner import get_script_run_ctx
    if get_script_run_ctx() is None:
        print("â— ì´ ì•±ì€ 'python app.py'ê°€ ì•„ë‹ˆë¼ 'streamlit run app.py' ë¡œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")
        raise SystemExit(1)
except Exception:
    pass

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ App Config â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="StudyMind AI", page_icon="ğŸ§ ", layout="wide")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒŸ Pastel Purple Apple-like CSS (v2.6: í°íŠ¸ ì¶•ì†Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASTEL_CSS = """
<style>
  :root{
    --accent:#7C3AED; --accent-2:#A78BFA;
    --ink-900:#12121A; --ink-700:#2E2A3B; --ink-500:#6D6A75; --ink-300:#A8A5AE;
    --surface:#ffffff; --surface-2:#F7F6FB; --line:#E9E7F2; --radius:16px;
    --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
    --shadow-accent: 0 10px 28px rgba(124,58,237,0.12);
  }
  html, body, [data-testid="stAppViewContainer"], [data-testid="stApp"]{
    font-family:-apple-system,BlinkMacSystemFont,"SF Pro Text","SF Pro Display",
      "Helvetica Neue",Helvetica,Arial,"Segoe UI",Roboto,system-ui,sans-serif !important;
    color:var(--ink-900);
    background: linear-gradient(160deg, #F5F3FF 0%, #FFFFFF 60%);
  }
  h1 {
      color: var(--accent) !important;
      font-weight: 700 !important;
  }
  .block-container{ max-width:1200px !important; padding:2.0rem 1.2rem !important; }
  [data-testid="stSidebar"]{ background:var(--surface); border-right:1px solid var(--line); }
  
  .apple-card{
    background:var(--surface);
    border:1px solid var(--line);
    border-radius:var(--radius);
    padding:1.0rem 1.0rem;
    box-shadow:0 8px 28px rgba(124,58,237,0.08);
    margin-bottom:1.0rem;
    transition: all 0.2s ease-in-out; 
  }
  .apple-card:hover {
      transform: translateY(-3px); 
      box-shadow: var(--shadow-accent); 
  }

  .hero{
    display:flex; align-items:center; gap:.9rem; padding:0.9rem 1.1rem;
    background:linear-gradient(135deg, rgba(167,139,250,.18), rgba(124,58,237,.08));
    border:1px solid var(--line); border-radius:18px;
  }
  .hero-badge{
    background:var(--accent); color:white; font-weight:700; padding:.28rem .6rem; border-radius:999px; font-size:.85rem;
    box-shadow:0 6px 20px rgba(124,58,237,.25);
  }
  .hero-text{ color:var(--ink-700); font-weight:600; letter-spacing:-0.01em;}
  .apple-divider{ height:1px; background:var(--line); margin:1.0rem 0; }
  .stButton > button{
    border-radius:12px; border:1px solid var(--line); background:#fff; color:var(--ink-900);
    padding:0.62rem 0.9rem; transition: all .15s ease; font-weight:600;
  }
  .stButton > button:hover{ box-shadow:0 8px 24px rgba(124,58,237,0.15); transform: translateY(-1px);}
  .stButton > button:focus{ outline:2px solid var(--accent); }
  .btn-primary > button{ background:var(--accent)!important; color:white!important; border-color:transparent!important; }
  textarea, input, .stTextInput input{
    border-radius:12px !important; border:1px solid var(--line) !important; background:var(--surface-2);
  }
  [data-testid="stFileUploader"]{
    border:1px dashed var(--line) !important; border-radius:14px !important; padding:0.9rem !important;
    background:var(--surface-2);
  }
  [data-testid="stDownloadButton"] button{
    border-radius:12px; border:1px solid var(--line); background:#fff; font-weight:600;
  }
  .mcq-container { font-size: 0.85em; } 

  [data-testid="stMarkdownContainer"] h2 {
      font-size: 1.4rem !important;
  }
  [data-testid="stMarkdownContainer"] h3 {
      font-size: 1.0rem !important;
  }
</style>
"""
st.markdown(PASTEL_CSS, unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒŸ UI í—¬í¼: Context Manager (v2) â”€â”€â”€â”€â”€â”€â”€â”€â”€
@contextmanager
def apple_card():
    st.markdown('<div class="apple-card">', unsafe_allow_html=True)
    try:
        yield
    finally:
        st.markdown('</div>', unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Secrets / Keys (ì›ë³¸ ìœ ì§€) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _safe_get_secret(name: str) -> str:
    v = os.getenv(name, "")
    if v: return v
    try:
        return st.secrets.get(name, "")
    except Exception:
        return ""

OPENAI_API_KEY = _safe_get_secret("OPENAI_API_KEY")
TAVILY_API_KEY = _safe_get_secret("TAVILY_API_KEY")
if not OPENAI_API_KEY:
    st.error("â— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. (.streamlit/secrets.toml ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì— ì„¤ì • í•„ìš”)")
    st.stop()


DEFAULT_MODEL = "gpt-4o-mini"
WORDS_PER_PAGE_DEFAULT = 500
client = OpenAI(api_key=OPENAI_API_KEY)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ OpenAI wrapper (v2.3) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat_complete(model: str, messages: List[Dict], temperature: float = 0.2):
    try:
        return client.chat.completions.create(model=model, messages=messages, temperature=temperature)
    except AttributeError:
        import openai as _oai
        _oai.api_key = OPENAI_API_KEY
        return _oai.ChatCompletion.create(model=model, messages=messages, temperature=temperature)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Utils (ì›ë³¸ ìœ ì§€) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def inject(template: str, content: str) -> str:
    return template.replace("{content}", content or "")

def _strip_fences(txt: str) -> str:
    t = (txt or "").strip()
    t = re.sub(r"^```(?:json)?", "", t).strip()
    t = re.sub(r"```$", "", t).strip()
    return t

def safe_json_loads(t: str):
    t = _strip_fences(t).replace("â€œ", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'")
    if t and "'" in t and '"' not in t: t = t.replace("'", '"')
    t = re.sub(r",\s*([\}\]])", r"\1", t)
    return json.loads(t)

def read_pdf(file) -> str:
    try:
        reader = PdfReader(file)
        return "\n".join([page.extract_text() or "" for page in reader.pages])
    except Exception:
        return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Web Search + Extraction (v2.5) â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=1800, show_spinner=False)
def tavily_search(query: str, max_results: int = 5) -> List[Dict]:
    if not TAVILY_API_KEY: return []
    try:
        r = requests.post(
            "https://api.tavily.com/search",
            headers={"Content-Type": "application/json"},
            json={"api_key": TAVILY_API_KEY, "query": query, "max_results": max_results, "search_depth": "basic", "include_answer": False},
            timeout=20,
        )
        r.raise_for_status() 
        data = r.json()
        return [{"title": it.get("title",""), "url": it.get("url",""), "snippet": it.get("content","")} for it in data.get("results", []) if it.get("url")]
    except Exception:
        return []

@st.cache_data(ttl=1800, show_spinner=False)
def ddg_search(query: str, max_results: int = 5) -> List[Dict]:
    if not DDG_AVAILABLE: return []
    try:
        with DDGS() as ddgs:
            res = ddgs.text(query, max_results=max_results)
        return [{"title": r.get("title",""), "url": r.get("href","") or r.get("url",""), "snippet": r.get("body","")} for r in (res or []) if (r.get("href") or r.get("url"))]
    except Exception:
        return []

@st.cache_data(ttl=1800, show_spinner=False)
def fetch_page_text(url: str, max_chars: int = 9000) -> str:
    try:
        html = requests.get(url, timeout=15, headers={"User-Agent": "Mozilla/5.0"}).text
        if TRA_AVAILABLE:
            txt = trafilatura.extract(html, include_comments=False, include_tables=False, no_fallback=False)
            if txt and len(txt) > 300: return txt[:max_chars]
        txt = re.sub(r"<[^>]+>", " ", html)
        txt = re.sub(r"\s+", " ", txt)
        return txt[:max_chars]
    except Exception:
        return ""

def summarize_for_queries(text: str, temperature: float = 0.2) -> List[str]:
    prompt = f"""ì•„ë˜ ê°•ì˜/ë…¸íŠ¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¸í„°ë„· ê²€ìƒ‰ìš© ì¿¼ë¦¬ 3ê°œë¥¼ í•œêµ­ì–´ë¡œ ì§§ê²Œ ë§Œë“¤ì–´ì¤˜.
ì¡°ê±´: í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬, ì„œë¡œ ë‹¤ë¥¸ ê´€ì /ì„¸ë¶€ ì£¼ì œ, ë”°ì˜´í‘œÂ·ë²ˆí˜¸ ì—†ì´ í•œ ì¤„ì— í•˜ë‚˜.
ì›ë¬¸:
{text[:4000]}
"""
    rsp = chat_complete(
        model=DEFAULT_MODEL,
        messages=[{"role":"system","content":"ê²€ìƒ‰ì–´ ìƒì„± ë³´ì¡°ì. ê°„ê²°Â·êµ¬ì²´Â·ìƒí˜¸ë‹¤ì–‘."},
                  {"role":"user","content":prompt}],
        temperature=temperature,
    )
    out = (rsp.choices[0].message.content or "").strip().splitlines()
    queries = [q.strip("-â€¢ ").strip() for q in out if q.strip()]
    return queries[:3] if queries else []

def web_search_and_gather(content: str, pct: int, budget_chars: int = 8000) -> Tuple[str, List[Dict]]:
    if pct <= 0: return "", []
    if pct > 0 and not TAVILY_API_KEY:
        return "", []
        
    queries = summarize_for_queries(content, temperature=0.2)
    picked, used, sources = [], 0, []
    for q in queries:
        hits = tavily_search(q, 5) 
        if not hits and DDG_AVAILABLE:
            hits = ddg_search(q, 5)
            
        for h in hits:
            url = h.get("url", "")
            if not url or any(s["url"] == url for s in sources): continue
            body = fetch_page_text(url)
            if not body or len(body) < 400: continue
            take_budget = max(0, int(budget_chars * (pct / 100.0)) - used)
            if take_budget <= 0: break
            take = body[: min(len(body), take_budget)]
            picked.append(f"[{h.get('title','')}] {url}\n{take}")
            sources.append({"title": h.get("title", ""), "url": url})
            used += len(take)
            if used >= int(budget_chars * (pct / 100.0)): break
        if used >= int(budget_chars * (pct / 100.0)): break
    return "\n\n",sources if not picked else ("\n\n".join(picked), sources)

def gpt_with_web_context(main_prompt: str, content: str, temperature: float) -> str:
    try:
        pct = st.session_state.get("ext_pct", 30)
        web_ctx, _ = web_search_and_gather(content or main_prompt, pct=pct)
        
        merged = f"[ì¸í„°ë„· ì™¸ë¶€ ì»¨í…ìŠ¤íŠ¸]\n{web_ctx}\n\n[ì‚¬ìš©ì ì œê³µ ìë£Œ]\n{inject(main_prompt, content)}" if web_ctx else inject(main_prompt, content)
        
        rsp = chat_complete(
            model=DEFAULT_MODEL,
            messages=[{"role":"system","content":"ë‹¹ì‹ ì€ ì‹ ì¤‘í•˜ê³  ì •í™•í•œ í•™ìŠµ ë¹„ì„œì…ë‹ˆë‹¤. ì™¸ë¶€ ì»¨í…ìŠ¤íŠ¸ëŠ” ì°¸ê³ ë¡œë§Œ ì‚¬ìš©í•˜ê³ , í•µì‹¬ì€ ì›ë¬¸ì— ê·¼ê±°í•´ êµ¬ì¡°í™”í•©ë‹ˆë‹¤."},
                      {"role":"user","content":merged}],
            temperature=temperature,
        )
        return (rsp.choices[0].message.content or "").strip()

    except AuthenticationError:
        st.error("âŒ OpenAI API Key ì˜¤ë¥˜: .streamlit/secrets.toml íŒŒì¼ì˜ OPENAI_API_KEYê°€ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return "" 
    except RateLimitError:
        st.error("âŒ OpenAI API ì”ì•¡/í•œë„ ì˜¤ë¥˜: OpenAI ê³„ì •ì˜ í¬ë ˆë”§(ì”ì•¡)ì´ ì†Œì§„ë˜ì—ˆê±°ë‚˜ ì›”ê°„ ì‚¬ìš© í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
        return "" 
    except OpenAIError as e:
        st.error(f"âŒ OpenAI API ì¼ë°˜ ì˜¤ë¥˜: {e}")
        return ""
    except Exception as e_general:
        st.error(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ìƒì„± ì˜¤ë¥˜: {e_general}")
        return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ JSON Fixers (v2.6: ë§ˆì¸ë“œë§µ ë¦¬ìŠ¤íŠ¸ í”½ì„œ ì¶”ê°€) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fix_cornell_json(text: str) -> dict:
    t = _strip_fences(text).replace("â€œ", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'")
    if t.lstrip().startswith('"label"') or t.lstrip().startswith("'label'"): t = "{ " + t + " }"
    m = re.search(r"(\{.*\})", t, flags=re.DOTALL)
    if m: t = m.group(1)
    t = re.sub(r",\s*([\}\]])", r"\1", t)
    if "'" in t and '"' not in t: t = t.replace("'", '"')
    obj = json.loads(t)
    obj.setdefault("label", "(íŒŒì¼ëª… ë¯¸ìƒ)")
    obj.setdefault("title", "ê°•ì˜ ìš”ì•½")
    obj.setdefault("key_terms", [])
    obj.setdefault("notes", [])
    obj.setdefault("summary", "")
    if not isinstance(obj["key_terms"], list): obj["key_terms"] = [str(obj["key_terms"])]
    if not isinstance(obj["notes"], list): obj["notes"] = [str(obj["notes"])]
    return obj

def fix_mindmap_json(text: str) -> dict:
    t = _strip_fences(text).replace("â€œ", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'")
    if t.lstrip().startswith('"root"') or t.lstrip().startswith("'root'"): t = "{ " + t + " }"
    m = re.search(r"(\{.*\}|\[.*\])", t, flags=re.DOTALL)
    if m: t = m.group(1)
    t = re.sub(r",\s*([\}\]])", r"\1", t)
    if "'" in t and '"' not in t: t = t.replace('"','\\"').replace("'", '"')
    try:
        obj = json.loads(t)
    except Exception:
        m2 = re.search(r"(\{.*\})", t, flags=re.DOTALL)
        obj = json.loads(m2.group(1)) if m2 else {}
    if isinstance(obj, list) and obj and isinstance(obj[0], dict): obj = obj[0]
    if not (isinstance(obj, dict) and "root" in obj): raise ValueError("Mindmap JSON êµ¬ì¡° ì˜¤ë¥˜")
    obj.setdefault("children", [])
    return obj

def fix_mindmap_json_list(text: str) -> List[Dict]:
    t = _strip_fences(text).replace("â€œ", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'")
    t = re.sub(r",\s*([\}\]])", r"\1", t)
    arr = json.loads(t)
    
    if not isinstance(arr, list):
        raise ValueError("AI ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸(List) í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
    
    validated_list = []
    for item in arr:
        if not isinstance(item, dict) or "label" not in item or "map" not in item:
            continue
        try:
            map_str = json.dumps(item["map"])
            map_obj = fix_mindmap_json(map_str)
            item["map"] = map_obj
            validated_list.append(item)
        except Exception:
            continue
    return validated_list


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒŸ Word styling helpers (v2: í•œê¸€ ê¹¨ì§ ìˆ˜ì •) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def style_document_defaults(doc: Document):
    try:
        style = doc.styles['Normal']
        font = style.font
        font.name = "Gulim"; font.size = Pt(11)
        try:
            style._element.rPr.rFonts.set(qn('w:eastAsia'), 'Gulim')
        except Exception:
            pass 

        pf = style.paragraph_format; pf.space_before = Pt(0); pf.space_after = Pt(6); pf.line_spacing = 1.3
        
        for lvl, sz in [(1, 18), (2, 16), (3, 14)]:
            h = doc.styles[f'Heading {lvl}']
            h.font.name = "Gulim"; h.font.size = Pt(sz)
            try:
                h._element.rPr.rFonts.set(qn('w:eastAsia'), 'Gulim')
            except Exception:
                pass
    except Exception:
        pass 

def ensure_table_style(doc: Document, style_name: str = TABLE_STYLE_NAME):
    try:
        _ = doc.styles[style_name]
    except KeyError:
        pass

def _set_cell_text(cell, text: str, bold: bool = False, align=WD_ALIGN_PARAGRAPH.LEFT):
    cell.text = ""
    p = cell.paragraphs[0]; p.alignment = align
    run = p.add_run(str(text) if text is not None else "")
    run.bold = bold; run.font.size = Pt(11); run.font.name = "Gulim"
    try: run._element.rPr.rFonts.set(qn('w:eastAsia'), 'Gulim')
    except Exception: pass

def _style_header_row(row):
    for cell in row.cells:
        props = cell._tc.get_or_add_tcPr()
        shd = OxmlElement('w:shd'); shd.set(qn('w:fill'), "EEE9FF")
        props.append(shd)
        for p in cell.paragraphs:
            for r in p.runs: r.bold = True

def _set_col_widths(table, widths_inch: List[float]):
    table.autofit = False
    for row in table.rows:
        for i, w in enumerate(widths_inch):
            row.cells[i].width = Inches(w)
    table.alignment = WD_TABLE_ALIGNMENT.LEFT

def add_heading_ea(doc: Document, text: str, level: int = 1):
    p = doc.add_heading(level=level)
    run = p.add_run(str(text))
    run.font.name = "Gulim"; run.font.size = Pt(18 if level==1 else 16 if level==2 else 14)
    try: run._element.rPr.rFonts.set(qn('w:eastAsia'), 'Gulim')
    except Exception: pass

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒŸ Word exports (v2.11: Pillow ìŠ¤ì¼€ì¼ë§) â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _create_base_doc(title_suffix: str) -> Document:
    doc = Document()
    style_document_defaults(doc) 
    add_heading_ea(doc, f"StudyMind AI â€” {title_suffix}", level=1)
    doc.add_paragraph("")
    ensure_table_style(doc)
    return doc

def _save_doc_to_bytes(doc: Document) -> bytes:
    buf = io.BytesIO()
    doc.save(buf)
    buf.seek(0)
    return buf.read()

def make_cornell_docx_per_files(items: List[Tuple[str, dict]]) -> bytes:
    doc = _create_base_doc("ì½”ë„¬ì‹ ë…¸íŠ¸ ì •ë¦¬(ì²¨ë¶€íŒŒì¼ë³„)")

    for idx, (label, c) in enumerate(items, 1):
        add_heading_ea(doc, f"{idx}. {label}", level=2)
        p = doc.add_paragraph(); r = p.add_run(f"ì œëª©: {c.get('title','')}")
        r.font.name="Gulim"; r.bold=True
        doc.add_paragraph("")

        if c.get("key_terms"):
            p = doc.add_paragraph(); r = p.add_run("Key Terms"); r.bold=True
            t = doc.add_table(rows=1, cols=3, style=TABLE_STYLE_NAME)
            _style_header_row(t.rows[0])
            _set_cell_text(t.rows[0].cells[0], "í‚¤ì›Œë“œ1", True, WD_ALIGN_PARAGRAPH.CENTER)
            _set_cell_text(t.rows[0].cells[1], "í‚¤ì›Œë“œ2", True, WD_ALIGN_PARAGRAPH.CENTER)
            _set_cell_text(t.rows[0].cells[2], "í‚¤ì›Œë“œ3", True, WD_ALIGN_PARAGRAPH.CENTER)
            _set_col_widths(t, [1.2, 2.0, 2.6])
            row=None
            for i, kw in enumerate(c["key_terms"]):
                if i % 3 == 0: row = t.add_row()
                _set_cell_text(row.cells[i % 3], kw)
            doc.add_paragraph("")

        p = doc.add_paragraph(); r = p.add_run("Notes"); r.bold=True
        
        notes_list = c.get("notes", []) or []
        if not notes_list:
            doc.add_paragraph("(ë‚´ìš© ì—†ìŒ)")
        
        for note_text in notes_list:
            p_note = doc.add_paragraph(str(note_text), style="List Bullet")
            pf = p_note.paragraph_format
            pf.space_before = Pt(0); pf.space_after = Pt(3) 
            for r in p_note.runs:
                r.font.name = "Gulim"; r.font.size = Pt(11)
                try: r._element.rPr.rFonts.set(qn('w:eastAsia'), 'Gulim')
                except Exception: pass
        
        doc.add_paragraph("") 
        p = doc.add_paragraph(); r = p.add_run("Summary"); r.bold=True
        t = doc.add_table(rows=1, cols=1, style=TABLE_STYLE_NAME)
        _style_header_row(t.rows[0]); _set_cell_text(t.rows[0].cells[0], c.get("summary",""))
        _set_col_widths(t, [5.8])
        doc.add_page_break()

    return _save_doc_to_bytes(doc)

# ğŸŒŸ FIX (Req #1, #3 v2.11): ë§ˆì¸ë“œë§µ Word ìŠ¤ì¼€ì¼ë§ (TD, 6.5x9.5 ë¹„ìœ¨ìœ ì§€)
def _mm_to_dot(tree: dict) -> str:
    lines = []
    lines.append('digraph G {')
    # ğŸŒŸ (Req #1) rankdir=TD (ì„¸ë¡œ), (Req #3) size="6.5,9.5" (ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë°•ìŠ¤ì— ë§ì¶¤)
    lines.append('graph [rankdir=TD, fontsize=12, size="6.5,9.5"];')
    lines.append('node [shape=box, style="rounded,filled", fillcolor="#F6F2FF", color="#D6CCFF", fontname="Gulim", fontsize=12];')
    lines.append('edge [color="#C4B5FD", arrowsize=0.7];')
    def walk(node: dict, parent_id: str):
        # ğŸŒŸ (Req #3) TD ë ˆì´ì•„ì›ƒì— ë§ê²Œ ì¤„ë°”ê¿ˆ ë„ˆë¹„ 20ìœ¼ë¡œ
        nlabel_raw = node.get("root") or node.get("name") or "ë…¸ë“œ"
        nlabel = textwrap.fill(nlabel_raw, width=20).replace("\n", "\\n")
        
        nid = "n_" + uuid.uuid4().hex[:8]
        lines.append(f'{nid} [label="{nlabel}"];')
        if parent_id: lines.append(f'{parent_id} -> {nid};')
        for ch in node.get("children", []): walk(ch, nid)
        
    root_label_raw = tree.get("root","ì£¼ì œ")
    root_label = textwrap.fill(root_label_raw, width=20).replace("\n", "\\n")
    root_id = "n_" + uuid.uuid4().hex[:8]
    lines.append(f'{root_id} [label="{root_label}"];')
    
    for ch in tree.get("children", []): walk(ch, root_id)
    lines.append("}")
    return "\n".join(lines)

def make_mindmap_docx_per_file(items: List[Dict]) -> bytes:
    doc = _create_base_doc("ë§ˆì¸ë“œë§µ(íŒŒì¼ë³„)")

    for idx, item in enumerate(items, 1):
        label = item.get("label", f"ë§ˆì¸ë“œë§µ {idx}")
        tree = item.get("map", {"root": "ì˜¤ë¥˜", "children": []})
        
        add_heading_ea(doc, f"{idx}. {label}", level=2)
        doc.add_paragraph("")
        
        try:
            if GV_AVAILABLE and PIL_AVAILABLE:
                dot = _mm_to_dot(tree) # ğŸŒŸ size="6.5,9.5" ì†ì„±ì´ í¬í•¨ëœ dot ìƒì„±
                src = gv.Source(dot)
                png_bytes = src.pipe(format="png")
                
                # ğŸŒŸ FIX (Req #3 v2.11): Pillowë¡œ ì´ë¯¸ì§€ ë¹„ìœ¨ ê³„ì‚°
                img_stream = io.BytesIO(png_bytes)
                img = Image.open(img_stream)
                img.close()
                width_px, height_px = img.size
                
                # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
                if width_px == 0 or height_px == 0:
                    raise ValueError("ì´ë¯¸ì§€ í¬ê¸° 0")

                aspect_ratio = float(height_px) / float(width_px)
                
                max_width_in = 6.5
                max_height_in = 9.5
                
                # ë„ˆë¹„ë¥¼ 6.5ë¡œ ê³ ì •í–ˆì„ ë•Œì˜ ì˜ˆìƒ ë†’ì´
                target_width = Inches(max_width_in)
                target_height = target_width * aspect_ratio
                
                if target_height > Inches(max_height_in):
                    # ë„ˆë¬´ ê¹€ -> ë†’ì´ë¥¼ 9.5ë¡œ ê³ ì •
                    doc.add_picture(io.BytesIO(png_bytes), height=Inches(max_height_in))
                else:
                    # ì ì ˆí•¨ -> ë„ˆë¹„ë¥¼ 6.5ë¡œ ê³ ì •
                    doc.add_picture(io.BytesIO(png_bytes), width=Inches(max_width_in))
                
                doc.paragraphs[-1].alignment = WD_ALIGN_PARAGRAPH.CENTER
            else:
                doc.add_paragraph("(Graphviz ë˜ëŠ” Pillow ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.)")
        except Exception as e_gv:
            doc.add_paragraph(f"(ë§ˆì¸ë“œë§µ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e_gv})")
        
        doc.add_page_break()

    return _save_doc_to_bytes(doc)


def make_quiz_docx(quiz: List[dict], choices: Dict[int, str], score: int) -> bytes:
    doc = _create_base_doc("4ì§€ì„ ë‹¤ í€´ì¦ˆ ê²°ê³¼")

    for idx, q in enumerate(quiz, 1):
        add_heading_ea(doc, f"ë¬¸í•­ {idx}. {q.get('question','')}", level=2)
        t = doc.add_table(rows=1, cols=2, style=TABLE_STYLE_NAME)
        _style_header_row(t.rows[0])
        _set_cell_text(t.rows[0].cells[0], "ë³´ê¸°", True, WD_ALIGN_PARAGRAPH.CENTER)
        _set_cell_text(t.rows[0].cells[1], "ë‚´ìš©", True)
        _set_col_widths(t, [0.9, 5.1])

        opts = q.get("options", [])
        ans = q.get("answer", "")
        for i, o in enumerate(opts):
            r = t.add_row().cells
            label = chr(ord('A') + i)
            _set_cell_text(r[0], label, bold=True, align=WD_ALIGN_PARAGRAPH.CENTER)
            _set_cell_text(r[1], o, bold=(label == ans))

        sel = choices.get(idx, "")
        doc.add_paragraph(f"ì„ íƒ: {sel if sel else '(ë¯¸ì„ íƒ)'} / ì •ë‹µ: {ans}")
        exp = q.get("explanation", "")
        if exp: doc.add_paragraph(f"í•´ì„¤: {exp}")
        srcs = q.get("sources") or []
        if srcs:
            doc.add_paragraph("ì°¸ê³  ë§í¬:")
            for s in srcs[:5]:
                p = doc.add_paragraph(style=None)
                r = p.add_run(f"- {s.get('title','link')}: {s.get('url','')}")
                r.font.name="Gulim"
                try: r._element.rPr.rFonts.set(qn('w:eastAsia'), 'Gulim')
                except Exception: pass

    doc.add_paragraph("")
    add_heading_ea(doc, f"ì´ì : {score}/{len(quiz)}", level=2)
    
    return _save_doc_to_bytes(doc)

def make_flashcards_docx(cards: List[dict]) -> bytes:
    doc = _create_base_doc("í”Œë˜ì‹œì¹´ë“œ")

    for idx, c in enumerate(cards, 1):
        add_heading_ea(doc, f"ì¹´ë“œ {idx}: {c.get('front','')}", level=2)
        
        p_ans = doc.add_paragraph()
        p_ans.add_run("ì •ë‹µ: ").bold = True
        r_ans = p_ans.add_run(c.get("back",""))
        r_ans.font.name = "Gulim"
        try: r_ans._element.rPr.rFonts.set(qn('w:eastAsia'), 'Gulim')
        except Exception: pass
        
        p_exp = doc.add_paragraph()
        p_exp.add_run("í•´ì„¤: ").bold = True
        for line in (c.get("explain","") or "").splitlines():
            r_exp = p_exp.add_run(line + "\n")
            r_exp.font.name = "Gulim"
            try: r._element.rPr.rFonts.set(qn('w:eastAsia'), 'Gulim')
            except Exception: pass

        srcs = c.get("sources") or []
        if srcs:
            p_src_header = doc.add_paragraph()
            p_src_header.add_run("ì°¸ê³  ë§í¬:").bold = True
            for s in srcs[:5]:
                link_text = f"- {s.get('title','link')}: {s.get('url','')}"
                p_link = doc.add_paragraph(link_text, style="List Bullet")
                for r in p_link.runs: 
                    r.font.name = "Gulim"
                    try: r._element.rPr.rFonts.set(qn('w:eastAsia'), 'Gulim')
                    except Exception: pass
        
        p_file = doc.add_paragraph()
        r_file = p_file.add_run(f"(ì¶œì²˜ íŒŒì¼: {c.get('from_file','')})")
        r_file.italic = True
        r_file.font.name = "Gulim"
        try: r_file._element.rPr.rFonts.set(qn('w:eastAsia'), 'Gulim')
        except Exception: pass

        if idx < len(cards):
            doc.add_paragraph("---")

    return _save_doc_to_bytes(doc)


def make_markdownish_docx(title: str, text: str, target_pages: int = 3, words_per_page: int = 500) -> bytes:
    doc = _create_base_doc(title) 

    lines = (text or "").splitlines()
    word_count, pages_inserted = 0, 0
    
    in_table = False
    table_data = []

    def flush_table(doc, table_data):
        if not table_data:
            return
        
        cols = len(table_data[0])
        try:
            table = doc.add_table(rows=0, cols=cols, style=TABLE_STYLE_NAME)
            table.autofit = True 
            
            for i, row_cells in enumerate(table_data):
                row = table.add_row()
                is_header = (i == 0) 
                if is_header:
                    _style_header_row(row)
                    
                for j, cell_text in enumerate(row_cells):
                    align = WD_ALIGN_PARAGRAPH.CENTER if is_header else WD_ALIGN_PARAGRAPH.LEFT
                    _set_cell_text(row.cells[j], cell_text, bold=is_header, align=align)
            doc.add_paragraph("") 
        except Exception as e:
            st.warning(f"Word í‘œ ìƒì„± ì‹¤íŒ¨: {e}")
        table_data.clear()

    def add_para(content: str, bullet: bool = False):
        nonlocal word_count, pages_inserted
        content = content.rstrip()
        if bullet:
            p = doc.add_paragraph(content, style="List Bullet")
        else:
            p = doc.add_paragraph(content)
        pf = p.paragraph_format
        pf.space_before = Pt(0); pf.space_after = Pt(6); pf.line_spacing = 1.3
        
        for r in p.runs:
            r.font.name = "Gulim"; r.font.size = Pt(11)
            try: r._element.rPr.rFonts.set(qn('w:eastAsia'), 'Gulim')
            except Exception: pass
            
        word_count += len(content.split())
        if word_count >= words_per_page:
            doc.add_page_break()
            word_count = 0; pages_inserted += 1

    for raw in lines:
        s = raw.strip()

        if s.startswith("|") and s.endswith("|"):
            cells = [c.strip() for c in s[1:-1].split("|")]
            if not all(re.match(r"^-+$", c) for c in cells):
                if not in_table:
                    in_table = True 
                    table_data.append(cells)
                elif len(cells) == len(table_data[0]): 
                    table_data.append(cells)
            continue 
        
        if in_table:
            flush_table(doc, table_data)
            in_table = False
            if not s: continue

        if not s:
            doc.add_paragraph("")
            continue
        if s.startswith("### "):
            add_heading_ea(doc, s[4:].strip(), level=3); continue
        if s.startswith("## "):
            add_heading_ea(doc, s[3:].strip(), level=2); continue
        if s.startswith("# "):
            add_heading_ea(doc, s[2:].strip(), level=1); continue
        if re.match(r"^\s*[-*]\s+", s):
            add_para(re.sub(r"^\s*[-*]\s+", "", s), bullet=True)
        else:
            add_para(s)
    
    if in_table:
        flush_table(doc, table_data)

    while pages_inserted < (target_pages - 1):
        doc.add_page_break(); pages_inserted += 1

    return _save_doc_to_bytes(doc)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒŸ Prompts (v2.9: ë§ˆì¸ë“œë§µ 4ë‹¨ê³„+) â”€â”€â”€â”€â”€â”€â”€â”€â”€

PROMPT_CORNELL_JSON = """[ë§¤ìš° ì¤‘ìš”] í˜„ì¬ë³´ë‹¤ 3ë°° ë” ê¸¸ê³  ìƒì„¸í•œ ë‚´ìš©ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
[ë§¤ìš° ì¤‘ìš”] **íŒŒì¼ë³„ë¡œ ê°ê°** ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. **ì ˆëŒ€ í•˜ë‚˜ë¡œ í•©ì¹˜ì§€ ë§ˆì„¸ìš”.**
'notes'ëŠ” ìµœì†Œ 10-15ê°œ, 'summary'ëŠ” 9-15ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.

ì•„ë˜ 'ìë£Œ i: íŒŒì¼ëª…'ë³„ ì›ë¬¸ì„ ê°ê° ì½”ë„¬ì‹ ë…¸íŠ¸ JSONìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.
ë°˜í™˜ì€ ë¦¬ìŠ¤íŠ¸:
[
  {"label":"ìë£Œ 1: íŒŒì¼ëª….pdf","title":"ì œëª©",
   "key_terms":["í‚¤1","í‚¤2","í‚¤3", "í‚¤4", "í‚¤5"],
   "notes":["[í•„ìˆ˜] ë§¤ìš° ìƒì„¸í•œ ìš”ì  1 (ìµœì†Œ 2-3ë¬¸ì¥)", "[í•„ìˆ˜] ë§¤ìš° ìƒì„¸í•œ ìš”ì  2 (ìµœì†Œ 2-3ë¬¸ì¥)", "... (ì´ 10-15ê°œ ì´ìƒ)"],
   "summary":"[í•„ìˆ˜] 9~15ë¬¸ì¥ ì´ìƒì˜ ë§¤ìš° ìƒì„¸í•œ ìš”ì•½"}
  , ...
]
ìë£Œ:
{content}
"""

# ğŸŒŸ FIX (Req #2 v2.9): ë§ˆì¸ë“œë§µ 3-4ë‹¨ê³„+ ì˜ˆì‹œ ë° ì§€ì‹œì–´
PROMPT_MINDMAP_JSON_PER_FILE = """[ë§¤ìš° ì¤‘ìš”] ì•„ë˜ 'ìë£Œ i: íŒŒì¼ëª…'ë³„ ì›ë¬¸ì„ **ê°ê°** ë³„ê°œì˜ ë§ˆì¸ë“œë§µ JSONìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.
[ë§¤ìš° ì¤‘ìš”] **ì ˆëŒ€ í•˜ë‚˜ë¡œ í•©ì¹˜ì§€ ë§ˆì„¸ìš”.**
[ë§¤ìš° ì¤‘ìš”] ë§ˆì¸ë“œë§µì€ **ìµœì†Œ 3ë‹¨ê³„ì—ì„œ 4ë‹¨ê³„ ì´ìƒ ê¹Šì´(root -> ëŒ€ë¶„ë¥˜ -> ì¤‘ë¶„ë¥˜ -> ì†Œë¶„ë¥˜...)**ë¡œ ë§¤ìš° ìƒì„¸í•˜ê²Œ êµ¬ì„±í•˜ì„¸ìš”.

ë°˜í™˜ì€ ë°˜ë“œì‹œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ì–´ì•¼ í•¨:
[
  {"label":"ìë£Œ 1: íŒŒì¼ëª….pdf",
   "map": {
     "root": "ìë£Œ 1ì˜ í•µì‹¬ ì£¼ì œ",
     "children": [
       {"name":"ëŒ€ë¶„ë¥˜ 1", "children": [
         {"name": "ì¤‘ë¶„ë¥˜ 1-1", "children": [
           {"name": "ì†Œë¶„ë¥˜ 1-1-1"},
           {"name": "ì†Œë¶„ë¥˜ 1-1-2"}
         ]},
         {"name": "ì¤‘ë¶„ë¥˜ 1-2", "children": [
           {"name": "ì†Œë¶„ë¥˜ 1-2-1"}
         ]}
       ]},
       {"name":"ëŒ€ë¶„ë¥˜ 2", "children": [
         {"name": "ì¤‘ë¶„ë¥˜ 2-1"}
       ]}
     ]
   }
  },
  {"label":"ìë£Œ 2: ë…¸íŠ¸.txt",
   "map": {
     "root": "ìë£Œ 2ì˜ í•µì‹¬ ì£¼ì œ",
     "children": [...]
   }
  }
]
ìë£Œ:
{content}
"""

PROMPT_FLASHCARDS_JSON = """ì•„ë˜ í†µí•© ìë£Œ(íŒŒì¼ëª… ë¼ë²¨ í¬í•¨)ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”Œë˜ì‹œì¹´ë“œ **ì •í™•íˆ {n}ì¥**ì„ JSON ë°°ì—´ë¡œ ë§Œë“¤ì–´ì¤˜.
ê° ì¹´ë“œì—ëŠ” ë‹¤ìŒ í•„ë“œê°€ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨:
- "front": "ì§ˆë¬¸"
- "back": "ì •ë‹µ"
- "explain": "**ìµœì†Œ 5ì¤„ ì´ìƒ**ì˜ ìì„¸í•œ ì„¤ëª…(í•µì‹¬ ê°œë…/ê·¼ê±°/ë¹„êµ/ì˜ˆì‹œ í¬í•¨, ì¸í„°ë„· ìë£Œ ë°˜ì˜)"
- "from_file": "ì–´ëŠ ìë£Œ ì¶œì²˜ì¸ì§€(ì˜ˆ: ìë£Œ 2: íŒŒì¼ëª….pdf)"
- "sources": [ì¤‘ìš”] **ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ”** ìë£Œë§Œ í¬í•¨. **ì ˆëŒ€ ê°€ì§œ ë§í¬ ë§Œë“¤ì§€ ë§ ê²ƒ.** [{"title":"...","url":"..."}]
[
  {"front":"ì§ˆë¬¸","back":"ì •ë‹µ","explain":"5ì¤„ ì´ìƒ ìƒì„¸ì„¤ëª…","from_file":"ìë£Œ 1: ...","sources":[{"title":"...","url":"..."}]}
]
ìë£Œ:
{content}
"""

PROMPT_QUIZ_JSON = """ì•„ë˜ í†µí•© ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ 4ì§€ì„ ë‹¤ {n}ë¬¸í•­ JSON ìƒì„±:
ê° ë¬¸í•­ì€ ë‹¤ìŒ í•„ë“œë¥¼ ë°˜ë“œì‹œ í¬í•¨:
- "question": "ì§ˆë¬¸"
- "options": ["Aë³´ê¸°","Bë³´ê¸°","Cë³´ê¸°","Dë³´ê¸°"]
- "answer": "A|B|C|D"
- "explanation": "**ìµœì†Œ 5ì¤„ ì´ìƒ**ì˜ ë§¤ìš° ìì„¸í•œ í•´ì„¤(ê·¼ê±°Â·ì •ì˜Â·ì˜ˆì‹œÂ·ë¹„êµ) + ê°€ëŠ¥í•˜ë©´ ì¸í„°ë„· ì¶œì²˜ ë°˜ì˜"
- "sources": [ì¤‘ìš”] **ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼ì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ”** ìë£Œë§Œ í¬í•¨. **ì ˆëŒ€ ê°€ì§œ ë§í¬ ë§Œë“¤ì§€ ë§ ê²ƒ.** [{"title":"...","url":"..."}]
í˜•ì‹:
[
  {"question":"...","options":["...","...","...","..."],"answer":"B","explanation":"5ì¤„ ì´ìƒ","sources":[{"title":"...","url":"..."}]}
]
ìë£Œ:
{content}
"""

PROMPT_EXAM = """ì•„ë˜ í†µí•© ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ **í˜„ì¬ë³´ë‹¤ 3ë°° ë” ê¸¸ê³  ìƒì„¸í•œ** ì‹œí—˜ ëŒ€ë¹„ í•µì‹¬ ìš”ì•½(ë¶ˆë¦¿ **20~30ê°œ**)ê³¼ ì˜ˆìƒë¬¸ì œ 5ê°œë¥¼ ìƒì„±.
ê° ì˜ˆìƒë¬¸ì œì—ëŠ” **í˜„ì¬ë³´ë‹¤ 3ë°° ë” ìì„¸í•˜ê³  ê¸´ ëª¨ë²”ë‹µì•ˆ**(ê·¼ê±°/ì ˆì°¨/ê³µì‹/ì˜ˆì‹œ í¬í•¨, ìµœì†Œ **12~24ë¬¸ì¥**)ì„ í•¨ê»˜ ì œì‹œ.
**ì „ì²´ ë¶„ëŸ‰ì´ í˜„ì¬ë³´ë‹¤ 3ë°° ì´ìƒ ê¸¸ì–´ì ¸ì•¼ í•¨.**
ìë£Œê°€ íŒŒì¼ë³„ë¡œ ë‹¤ë¥¸ ì£¼ì œë¥¼ í¬í•¨í•˜ë©´ **[í•„ìˆ˜] 'ìë£Œ i: íŒŒì¼ëª…' í˜•ì‹ì˜ ì†Œì œëª©**ì„ ë°˜ë“œì‹œ ë‹¬ê³ , ë‚´ìš©ì€ **í†µí•©Â·ëŒ€ì¡°Â·ì •ë¦¬**í•˜ë˜ ì¶œì²˜ êµ¬ë¶„ ëª…ì‹œ.
ìë£Œ:
{content}
"""

PROMPT_EXAM_LONG = """ì•„ë˜ í†µí•© ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ **[ë§¤ìš° ì¤‘ìš”] í˜„ì¬ë³´ë‹¤ 3ë°° ë” ê¸¸ê³  ë§¤ìš° ìƒì„¸í•œ ì‹œí—˜ ëŒ€ë¹„ ì¥ë¬¸ ìš”ì•½**ì„ ì‘ì„±í•˜ì„¸ìš”.
- ì²¨ë¶€ íŒŒì¼ì´ Nê°œë©´ ê° ì²¨ë¶€íŒŒì¼ ì„¹ì…˜ì— **í˜„ì¬ë³´ë‹¤ 3ë°° ë” ë§ì€** ì„¤ëª… ë¶„ëŸ‰ ë°°ì¹˜(ì´ ìµœì†Œ **{min_pages} í˜ì´ì§€, {min_words} ë‹¨ì–´** ì´ìƒ).
- **[í•„ìˆ˜] ë°˜ë“œì‹œ ì´ ë¶„ëŸ‰ì„ ì±„ì›Œì•¼ í•˜ë©°, ê° ì„¹ì…˜ì„ ë§¤ìš° ìƒì„¸í•˜ê³  ê¸¸ê²Œ ì„¤ëª…í•  ê²ƒ.**
- ê° ì²¨ë¶€íŒŒì¼ ì„¹ì…˜ì€ ê¼­ **"ìë£Œ i: íŒŒì¼ëª…"** í˜•ì‹ì˜ ì œëª©ê³¼ ë²ˆí˜¸ë¥¼ ë‹¬ ê²ƒ
- ì„œë¡œ ë‹¤ë¥¸ ì£¼ì œëŠ” íŒŒì¼ë³„ë¡œ ì •ë¦¬í•˜ë˜ **ê³µí†µì /ì°¨ì´/ìƒì¶©ë‚´ìš©**ì„ ëª…í™•íˆ ë¹„êµ
- êµ¬ì„±(ì˜ˆì‹œ):
  1) Executive Summary (í•µì‹¬ 15~25 ë¶ˆë¦¿)
  2) [ìë£Œ 1: íŒŒì¼ëª…] **(ì´ˆ ì¥ë¬¸)** ìƒì„¸ ìš”ì•½ (ì •ì˜/ì›ë¦¬/í•µì‹¬ê°œë…/ì ˆì°¨/ê³µì‹/ì˜ˆì‹œ/ì£¼ì˜ì )
  3) [ìë£Œ 2: íŒŒì¼ëª…] **(ì´ˆ ì¥ë¬¸)** ìƒì„¸ ìš”ì•½
  4) ...
  5) ìƒì¶©Â·í˜¼ë™ í¬ì¸íŠ¸(í…ìŠ¤íŠ¸ ì„¤ëª… ë˜ëŠ” | | | í˜•ì‹ì˜ í‘œ ì‚¬ìš©)
  6) ìì£¼ ë‚˜ì˜¤ëŠ” ì‹¤ìˆ˜/ì˜¤ê°œë…ê³¼ êµì • í¬ì¸íŠ¸
  7) **ì˜ˆìƒ ë¬¸ì œ 5ê°œ + ê° ë¬¸ì œì˜ (í˜„ì¬ë³´ë‹¤ 3ë°° ê¸´) ìì„¸í•œ ëª¨ë²”ë‹µì•ˆ**(12~24ë¬¸ì¥, ê·¼ê±°/ì ˆì°¨/ê³µì‹/ì˜ˆì‹œ í¬í•¨)
  8) 1ì£¼/3ì£¼/ì‹œí—˜ ì§ì „ í•™ìŠµ í”Œëœ(ë¶ˆë¦¿)
- í•œêµ­ì–´, ëª…í™•í•œ ì†Œì œëª©/ë¶ˆë¦¿ ì¤‘ì‹¬
ìë£Œ:
{content}
"""

PROMPT_MOOD = """ë‹¤ìŒ ì‚¬ìš©ìì˜ ì˜¤ëŠ˜ ê°ì •/ìƒí™©ì„ ì½ê³ ,
1) ë”°ëœ»í•œ ê³µê° í•œ ë¬¸ë‹¨
2) ì›ì¸ ê°€ì„¤ 2~3ê°œ
3) ë‹¹ì¥ 5ë¶„ ë£¨í‹´ (ì•„ì£¼ êµ¬ì²´ì )
4) ë‚´ì¼ì˜ ì‘ì€ ì‹¤ì²œ (ì¸¡ì • ê°€ëŠ¥í•œ í–‰ë™)
5) í•„ìš”ì‹œ ë„ì›€ìš”ì²­ ì‹ í˜¸ì™€ ì¼ë°˜ ë¦¬ì†ŒìŠ¤(í•œêµ­ ê¸°ì¤€)
í†¤: ì§„ì‹¬ ì–´ë¦° ì‘ì›, ê³¼ì¥Â·ì„¤êµ ê¸ˆì§€.
ì‚¬ìš©ì ì…ë ¥:
{content}
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sidebar (v2.5) â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.sidebar.title("ğŸ§  StudyMind AI")
st.sidebar.caption("Pastel-Purple Â· Minimal Â· Focused")

st.sidebar.markdown("### âš™ï¸ ì„¤ì •")
ext_pct = st.sidebar.slider("ì¸í„°ë„· ì™¸ë¶€ë°ì´í„° í™œìš© í¼ì„¼íŠ¸ %", 0, 100, 30, 10)
st.session_state["ext_pct"] = ext_pct
TEMPERATURE = ext_pct / 100.0
st.sidebar.caption(f"ëª¨ë¸: {DEFAULT_MODEL} Â· temperature={TEMPERATURE:.1f}")

st.sidebar.markdown("---")
page = st.sidebar.radio(
    "ğŸ“š ë©”ë‰´",
    [
        "ğŸ”¥ ì‹œí—˜ëŒ€ë¹„ í†µí•© ìš”ì•½",
        "ğŸŒ¿ ë§ˆì¸ë“œë§µ",
        "ğŸ’¡ í”Œë˜ì‹œì¹´ë“œ",
        "ğŸ§© 4ì§€ì„ ë‹¤ í€´ì¦ˆ",
        "ğŸ“„ ì½”ë„¬ì‹ ë…¸íŠ¸ ì •ë¦¬",
        "ğŸ’š ì˜¤ëŠ˜ì˜ ê°ì • ì½”ì¹­",
    ],
    index=0
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Header + Hero (ì›ë³¸ ìœ ì§€) â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header(page) 
st.markdown(
    """
    <div class="hero">
      <span class="hero-badge">STUDYMIND AI</span>
      <span class="hero-text">ë™ì„œìš¸ëŒ€í•™êµ <b>ìŠ¤ë§ˆíŠ¸ë“œë¡ ê³¼</b> Â· <b>ê¹€ë™ì—½</b> ì œì‘</span>
    </div>
    """,
    unsafe_allow_html=True
)

st.caption("í•„ìš” ìë£Œ(PDF/TXT ìµœëŒ€ 20ê°œ) ì—…ë¡œë“œ ë˜ëŠ” ë…¸íŠ¸ ë¶™ì—¬ë„£ê¸° â†’ ì•„ë˜ ê¸°ëŠ¥ ì¹´ë“œì—ì„œ ìƒì„±")
st.markdown('<div class="apple-divider"></div>', unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒŸ Inputs (v2.9: ë™ì  ìŠ¤í”¼ë„ˆìš© n_attachments ì¶”ê°€) â”€â”€â”€â”€â”€â”€â”€â”€â”€
colL, colR = st.columns([1, 1])
with colL:
    with apple_card():
        uploaded_files = st.file_uploader("ğŸ“ ê°•ì˜ìë£Œ ì—…ë¡œë“œ (PDF/TXT ìµœëŒ€ 20ê°œ)", type=["pdf","txt"], accept_multiple_files=True)
with colR:
    with apple_card():
        text_input = st.text_area("ğŸ“ ë…¸ì…˜/í•„ê¸° í…ìŠ¤íŠ¸",
                                    height=160,
                                    placeholder="ìˆ˜ì—…ì¤‘ í•„ê¸°í•œ ë©”ëª¨ ë‚´ìš©ì„ ë¶™ì—¬ì£¼ì„¸ìš”")

files = list(uploaded_files) if uploaded_files else []
if len(files) > 20:
    st.warning(f"ì—…ë¡œë“œ íŒŒì¼ì´ 20ê°œë¥¼ ì´ˆê³¼í•˜ì—¬ ì•ì˜ 20ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì´ {len(files)}ê°œ ì¤‘ 20ê°œ ì‚¬ìš©)")
    files = files[:20]

attachments: List[Tuple[str, str]] = []
if files:
    for idx, f in enumerate(files, 1):
        name = getattr(f, "name", f"attachment_{idx}")
        body = read_pdf(f) if f.type == "application/pdf" else f.read().decode("utf-8", errors="ignore")
        label = f"ìë£Œ {idx}: {name}"
        if body.strip():
            attachments.append((label, body.strip()))
if text_input.strip():
    attachments.append(("ì‚¬ìš©ì ë…¸íŠ¸", text_input.strip()))

n_attachments = max(1, len(attachments))

parts = [f"### [{label}]\n{txt}" for (label, txt) in attachments]
full_text = "\n\n".join(parts).strip()

if not full_text:
    st.info("ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë…¸íŠ¸ë¥¼ ë¶™ì—¬ë„£ìœ¼ë©´ ìƒì„± ê¸°ëŠ¥ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

st.markdown('<div class="apple-divider"></div>', unsafe_allow_html=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Primary button wrapper (ì›ë³¸ ìœ ì§€) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def primary_button(label: str, key: str = None):
    c = st.container()
    with c:
        st.markdown('<div class="btn-primary">', unsafe_allow_html=True)
        out = st.button(label, key=key, use_container_width=True)
        st.markdown('</div>', unsafe_allow_html=True)
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒŸ Pages (v2.9: ë™ì  ìŠ¤í”¼ë„ˆ, ì…ë ¥ ì—†ìŒ ê²½ê³ ) â”€â”€â”€â”€â”€â”€â”€â”€â”€

def show_no_input_warning():
    st.error("âš ï¸ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë…¸íŠ¸ë¥¼ ë¶™ì—¬ë„£ì€ í›„ ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")

# 1) Exam Summary
if page == "ğŸ”¥ ì‹œí—˜ëŒ€ë¹„ í†µí•© ìš”ì•½":
    with apple_card():
        st.caption("â€¢ ì²¨ë¶€ + ì›¹ì»¨í…ìŠ¤íŠ¸ ìë™ ë³‘í•© â€¢ íŒŒì¼ë³„ ì†Œì œëª©ìœ¼ë¡œ ëª…í™•íˆ êµ¬ë¶„ â€¢ ì˜ˆìƒë¬¸ì œ + ìì„¸í•œ ëª¨ë²”ë‹µì•ˆ í¬í•¨")
        base_pages = st.slider("ğŸ“„ ëª©í‘œ í˜ì´ì§€ ìˆ˜(ê°€ì´ë“œ)", 3, 10, 3, 1)

        colA, colB = st.columns(2)
        with colA:
            basic_ok = primary_button("í†µí•© ìš”ì•½ ìƒì„± (ê¸°ë³¸)", key="btn_exam_basic")
        with colB:
            long_ok = primary_button("í†µí•© ìš”ì•½ ìƒì„± (ìƒì„¸)", key="btn_exam_long")

        if basic_ok:
            if full_text:
                low, high = int(15 * n_attachments), int(30 * n_attachments)
                with st.spinner(f"AIê°€ ê¸°ë³¸ ìš”ì•½ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (ì•½ {low}~{high}ì´ˆ ì†Œìš”)"):
                    out = gpt_with_web_context(PROMPT_EXAM, full_text, temperature=TEMPERATURE)
                    if out: 
                        st.session_state["exam_text_basic"] = out
                        st.success("ê¸°ë³¸ í†µí•© ìš”ì•½ ìƒì„± ì™„ë£Œ!")
                        st.markdown(out)
            else:
                show_no_input_warning() 

        if long_ok:
            if full_text:
                low, high = int(30 * n_attachments), int(60 * n_attachments)
                with st.spinner(f"AIê°€ ìƒì„¸ ìš”ì•½ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (ì•½ {low}~{high}ì´ˆ ì†Œìš”)"):
                    min_pages = max(base_pages * 3, 1) * 3
                    min_words = min_pages * WORDS_PER_PAGE_DEFAULT
                    long_prompt = PROMPT_EXAM_LONG.format(min_words=min_words, min_pages=min_pages, content="{content}")
                    
                    out = gpt_with_web_context(long_prompt, full_text, temperature=TEMPERATURE)
                    if out: 
                        st.session_state["exam_text_long"] = out
                        st.success("ìƒì„¸ í†µí•© ìš”ì•½ ìƒì„± ì™„ë£Œ!")
                        st.markdown(out)
            else:
                show_no_input_warning() 

        st.markdown('<div class="apple-divider"></div>', unsafe_allow_html=True)
        colX, colY = st.columns(2)
        with colX:
            if st.session_state.get("exam_text_basic"):
                if primary_button("â¬‡ï¸ (ê¸°ë³¸) Word ë‚´ë³´ë‚´ê¸°", key="dl_exam_basic"):
                    docx_bytes = make_markdownish_docx(
                        "ì‹œí—˜ëŒ€ë¹„ í†µí•© ìš”ì•½(ê¸°ë³¸: ì˜ˆìƒë¬¸ì œ+ìì„¸í•œ ë‹µ í¬í•¨)",
                        st.session_state["exam_text_basic"],
                        target_pages=base_pages,
                        words_per_page=WORDS_PER_PAGE_DEFAULT
                    )
                    st.success("âœ… Word íŒŒì¼ ìƒì„± ì™„ë£Œ! ğŸ“„ ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                    st.download_button("ğŸ“¥ ë‹¤ìš´ë¡œë“œ", data=docx_bytes,
                        file_name="StudyMind_Exam_Summary_Basic.docx",
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        use_container_width=True)
        with colY:
            if st.session_state.get("exam_text_long"):
                if primary_button("â¬‡ï¸ (ìƒì„¸) Word ë‚´ë³´ë‚´ê¸°", key="dl_exam_long"):
                    docx_bytes = make_markdownish_docx(
                        "ì‹œí—˜ëŒ€ë¹„ í†µí•© ìš”ì•½(ìƒì„¸: ì˜ˆìƒë¬¸ì œ+ìì„¸í•œ ë‹µ í¬í•¨)",
                        st.session_state["exam_text_long"],
                        target_pages=base_pages*3, 
                        words_per_page=WORDS_PER_PAGE_DEFAULT
                    )
                    st.success("âœ… Word íŒŒì¼ ìƒì„± ì™„ë£Œ! ğŸ“„ ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                    st.download_button("ğŸ“¥ ë‹¤ìš´ë¡œë“œ", data=docx_bytes,
                        file_name="StudyMind_Exam_Summary_Detailed.docx",
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        use_container_width=True)

# 2) Mindmap
elif page == "ğŸŒ¿ ë§ˆì¸ë“œë§µ":
    with apple_card():
        st.caption("â€¢ [ì‹ ê·œ] ì²¨ë¶€íŒŒì¼ë§ˆë‹¤ ê°ê° ë§ˆì¸ë“œë§µ ìƒì„± â€¢ Word ë‚´ë³´ë‚´ê¸° ì§€ì›")
        ok = primary_button("ë§ˆì¸ë“œë§µ ìƒì„± (íŒŒì¼ë³„)", key="btn_mm_per_file")
        
        if ok:
            if full_text:
                low, high = int(15 * n_attachments), int(30 * n_attachments)
                with st.spinner(f"AIê°€ íŒŒì¼ë³„ ë§ˆì¸ë“œë§µì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (ì•½ {low}~{high}ì´ˆ ì†Œìš”)"):
                    raw = gpt_with_web_context(PROMPT_MINDMAP_JSON_PER_FILE, full_text, temperature=TEMPERATURE)
                    
                    if raw:
                        try:
                            mm_list = fix_mindmap_json_list(raw)
                            st.session_state["mindmaps_per_file"] = mm_list
                            st.success(f"ë§ˆì¸ë“œë§µ {len(mm_list)}ê°œ ìƒì„± ì„±ê³µ!")
                        except Exception as e_json:
                            st.error(f"AIê°€ ì‘ë‹µí–ˆìœ¼ë‚˜, JSON ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_json}")
                            st.error(f"AI ì›ë³¸ ì‘ë‹µ (ì¼ë¶€): {raw[:500]}...")
            else:
                show_no_input_warning() 

        mm_list = st.session_state.get("mindmaps_per_file", [])
        if mm_list:
            for i, item in enumerate(mm_list, 1):
                label = item.get("label", f"ë§ˆì¸ë“œë§µ {i}")
                tree = item.get("map", {"root": "ì˜¤ë¥˜"})
                
                st.markdown(f"### {i}. {label}")
                st.graphviz_chart(_mm_to_dot(tree)) 
                st.markdown('<div class="apple-divider"></div>', unsafe_allow_html=True)
        
            if primary_button("â¬‡ï¸ ë§ˆì¸ë“œë§µ (íŒŒì¼ë³„) Word ë‚´ë³´ë‚´ê¸°", key="dl_mm_per_file"):
                docx_bytes = make_mindmap_docx_per_file(mm_list)
                st.success("âœ… Word íŒŒì¼ ìƒì„± ì™„ë£Œ! ğŸ“„ ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                st.download_button("ğŸ“¥ ë‹¤ìš´ë¡œë“œ", data=docx_bytes,
                                  file_name="StudyMind_Mindmaps_PerFile.docx",
                                  mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                  use_container_width=True)

# 3) Flashcards
elif page == "ğŸ’¡ í”Œë˜ì‹œì¹´ë“œ":
    with apple_card():
        st.caption("â€¢ ì›¹ì»¨í…ìŠ¤íŠ¸ ìë™ ë³‘í•© â€¢ ì§ˆë¬¸ í´ë¦­ ì‹œ ë‹µë³€/í•´ì„¤ í¼ì¹˜ê¸° â€¢ Word (í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼) ë‚´ë³´ë‚´ê¸°")
        target_n = st.number_input("ìƒì„± ê°œìˆ˜", 5, 40, 10)
        ok = primary_button("í”Œë˜ì‹œì¹´ë“œ ìƒì„±", key="btn_cards")
        
        if ok:
            if full_text:
                low, high = int((15 * n_attachments) + target_n), int((30 * n_attachments) + (target_n * 2))
                with st.spinner(f"AIê°€ í”Œë˜ì‹œì¹´ë“œ {target_n}ê°œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (ì•½ {low}~{high}ì´ˆ ì†Œìš”)"):
                    debug_context = {"data": ""}
                    
                    try:
                        def gen_cards(n_needed: int) -> List[dict]:
                            n_to_request = n_needed + 1 
                            sub_prompt = PROMPT_FLASHCARDS_JSON.replace("{n}", str(n_to_request))
                            api_result = gpt_with_web_context(sub_prompt, full_text, temperature=TEMPERATURE)
                            if not api_result: 
                                return []
                            debug_context["data"] = api_result 
                            cards = safe_json_loads(api_result)
                            if isinstance(cards, dict): cards = [cards]
                            return cards

                        cards = gen_cards(int(target_n))
                        
                        if cards: 
                            seen = set()
                            unique_cards = []
                            for c in cards:
                                f = (c.get("front","") or "").strip()
                                if f and f not in seen:
                                    unique_cards.append(c)
                                    seen.add(f)
                            cards = unique_cards 

                            if len(cards) < int(target_n):
                                remain = int(target_n) - len(cards)
                                more = gen_cards(remain) 
                                for m in more:
                                    f = (m.get("front","") or "").strip()
                                    if f and f not in seen:
                                        cards.append(m); seen.add(f)
                            
                            cards = cards[:int(target_n)]

                            st.session_state["cards"] = cards
                            st.success(f"í”Œë˜ì‹œì¹´ë“œ {len(cards)}ê°œ ìƒì„± ì™„ë£Œ!")
                        
                    except Exception as e_json:
                        st.error(f"AIê°€ ì‘ë‹µí–ˆìœ¼ë‚˜, JSON ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_json}")
                        if debug_context["data"]:
                            st.error(f"AI ì›ë³¸ ì‘ë‹µ (ì¼ë¶€): {debug_context['data'][:500]}...")
            else:
                show_no_input_warning() 


        cards = st.session_state.get("cards") or []
        for i, c in enumerate(cards, 1):
            with st.expander(f"ì¹´ë“œ {i}: {c.get('front','(ì§ˆë¬¸)')}", expanded=False):
                st.caption(f"ì¶œì²˜ íŒŒì¼: {c.get('from_file','(ë¯¸ìƒ)')}")
                st.success(f"ì •ë‹µ: {c.get('back','')}")
                if c.get("explain"):
                    st.info(c["explain"])
                srcs = c.get("sources") or []
                if srcs:
                    st.markdown("**ì°¸ê³  ë§í¬**")
                    for s in srcs[:5]:
                        title = s.get("title") or s.get("url","link")
                        url = s.get("url","")
                        st.markdown(f"- [{title}]({url})")

        if cards:
            st.markdown('<div class="apple-divider"></div>', unsafe_allow_html=True)
            if primary_button("â¬‡ï¸ í”Œë˜ì‹œì¹´ë“œ Word ë‚´ë³´ë‚´ê¸°", key="dl_cards"):
                docx = make_flashcards_docx(cards)
                st.success("âœ… Word íŒŒì¼ ìƒì„± ì™„ë£Œ! ğŸ“„ ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                st.download_button("ğŸ“¥ ë‹¤ìš´ë¡œë“œ", data=docx, file_name="StudyMind_Flashcards.docx",
                                  mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                  use_container_width=True)

# 4) MCQ
elif page == "ğŸ§© 4ì§€ì„ ë‹¤ í€´ì¦ˆ":
    with apple_card():
        st.caption("â€¢ ì˜µì…˜ ì„ íƒ ì¦‰ì‹œ ì •Â·ì˜¤ë‹µë° í•´ì„¤ í‘œì‹œ â€¢ Word ë‚´ë³´ë‚´ê¸°")
        n_q = st.number_input("í€´ì¦ˆ ë¬¸í•­ ìˆ˜", 1, 20, 5, 1)
        ok = primary_button("í€´ì¦ˆ ìƒì„±", key="btn_quiz")
        
        if ok:
            if full_text:
                low, high = int((10 * n_attachments) + n_q), int((20 * n_attachments) + (n_q * 2))
                with st.spinner(f"AIê°€ í€´ì¦ˆ {n_q}ë¬¸í•­ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (ì•½ {low}~{high}ì´ˆ ì†Œìš”)"):
                    prompt = PROMPT_QUIZ_JSON.replace("{n}", str(n_q))
                    data = gpt_with_web_context(prompt, full_text, temperature=TEMPERATURE)
                    
                    if data:
                        try:
                            quiz = safe_json_loads(data)
                            st.session_state["quiz"] = quiz if isinstance(quiz, list) else [quiz]
                            st.session_state["quiz_choices"] = {}
                            st.session_state["quiz_score"] = 0
                            for idx in range(1, len(st.session_state["quiz"]) + 1):
                                st.session_state[f"graded_{idx}"] = False
                                st.session_state[f"correct_{idx}"] = False
                            st.success(f"í€´ì¦ˆ {len(st.session_state['quiz'])}ë¬¸í•­ ìƒì„± ì™„ë£Œ!")
                        except Exception as e_json:
                            st.error(f"AIê°€ ì‘ë‹µí–ˆìœ¼ë‚˜, JSON ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_json}")
                            st.error(f"AI ì›ë³¸ ì‘ë‹µ (ì¼ë¶€): {data[:500]}...")
            else:
                show_no_input_warning() 

        quiz = st.session_state.get("quiz") or []
        if quiz:
            def _grade_now(idx: int, answer: str, sel: str):
                if not sel: return
                st.session_state["quiz_choices"][idx] = sel
                st.session_state[f"graded_{idx}"] = True
                st.session_state[f"correct_{idx}"] = (sel == answer)

            score = 0
            for idx, q in enumerate(quiz, 1):
                st.markdown(f"### ë¬¸í•­ {idx}. {q.get('question','(ì§ˆë¬¸)')}")
                opts = q.get("options", ["A","B","C","D"])
                answer = q.get("answer", "")
                
                opts_list = [("A",opts[0]),("B",opts[1]),("C",opts[2]),("D",opts[3])]
                
                stored_choice_value = st.session_state.get("quiz_choices", {}).get(idx)
                
                default_index = None 
                if stored_choice_value:
                    for i, (val, label) in enumerate(opts_list):
                        if val == stored_choice_value:
                            default_index = i
                            break
                            
                with st.container():
                    st.markdown('<div class="mcq-container">', unsafe_allow_html=True)
                    sel = st.radio(
                        "ë³´ê¸° ì„ íƒ",
                        options=opts_list,
                        index=default_index, 
                        format_func=lambda x: f"{x[0]}. {x[1]}",
                        key=f"choice_{idx}"
                    )
                    
                    picked = sel[0] if isinstance(sel, tuple) else sel
                    
                    if picked:
                        _grade_now(idx, answer, picked)
                        
                    st.markdown('</div>', unsafe_allow_html=True)

                if st.session_state.get(f"graded_{idx}", False):
                    if st.session_state.get(f"correct_{idx}", False):
                        st.success("ì •ë‹µ! âœ…"); score += 1
                    else:
                        st.error(f"ì˜¤ë‹µ âŒ (ì •ë‹µ: {answer})")
                    exp = q.get("explanation")
                    if exp: st.info(f"í•´ì„¤:\n{exp}")
                    srcs = q.get("sources") or []
                    if srcs:
                        st.markdown("**ì°¸ê³  ë§í¬**")
                        for s in srcs[:5]:
                            title = s.get("title") or s.get("url","link")
                            url = s.get("url","")
                            st.markdown(f"- [{title}]({url})")

            st.session_state["quiz_score"] = score
            st.markdown(f"**ì´ì :** {score}/{len(quiz)}")

            st.markdown('<div class="apple-divider"></div>', unsafe_allow_html=True)
            if primary_button("â¬‡ï¸ í€´ì¦ˆ ê²°ê³¼ Word ë‚´ë³´ë‚´ê¸°", key="dl_quiz"):
                docx = make_quiz_docx(quiz, st.session_state.get("quiz_choices", {}), st.session_state.get("quiz_score", 0))
                st.success("âœ… Word íŒŒì¼ ìƒì„± ì™„ë£Œ! ğŸ“„ ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                st.download_button("ğŸ“¥ ë‹¤ìš´ë¡œë“œ", data=docx, file_name="StudyMind_Quiz_Result.docx",
                                  mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                  use_container_width=True)

# 5) Cornell per-file
elif page == "ğŸ“„ ì½”ë„¬ì‹ ë…¸íŠ¸ ì •ë¦¬":
    with apple_card():
        st.caption("â€¢ ì²¨ë¶€íŒŒì¼ë§ˆë‹¤ ê°ê° ìƒì„± â€¢ Word í‘œ ë‚´ë³´ë‚´ê¸°")
        ok = primary_button("ì½”ë„¬ ë…¸íŠ¸ ìƒì„±", key="btn_cornell")
        
        if ok:
            if full_text:
                low, high = int(20 * n_attachments), int(40 * n_attachments)
                with st.spinner(f"AIê°€ (ìƒì„¸í•œ) ì½”ë„¬ ë…¸íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (ì•½ {low}~{high}ì´ˆ ì†Œìš”)"):
                    raw = gpt_with_web_context(PROMPT_CORNELL_JSON, full_text, temperature=TEMPERATURE)
                    
                    if raw:
                        try:
                            arr = safe_json_loads(raw)
                            per_items: List[Tuple[str, dict]] = []
                            for obj in arr:
                                fixed = fix_cornell_json(json.dumps(obj))
                                label = fixed.get("label","(íŒŒì¼ëª… ë¯¸ìƒ)")
                                per_items.append((label, fixed))
                            st.session_state["cornell_per_files"] = per_items
                            st.success(f"ì½”ë„¬ ë…¸íŠ¸ {len(per_items)}ê°œ(íŒŒì¼ë³„) ìƒì„± ì™„ë£Œ!")
                        except Exception as e_json:
                            st.error(f"AIê°€ ì‘ë‹µí–ˆìœ¼ë‚˜, JSON ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_json}")
                            st.error(f"AI ì›ë³¸ ì‘ë‹µ (ì¼ë¶€): {raw[:500]}...")
            else:
                show_no_input_warning() 


        per_items = st.session_state.get("cornell_per_files") or []
        for i, (label, c) in enumerate(per_items, 1):
            st.markdown(f"### {i}. {label}")
            st.markdown(f"**ì œëª©:** {c.get('title','')}")
            st.markdown("**Key Terms:** " + (", ".join(c.get("key_terms",[])) if c.get("key_terms") else "(ì—†ìŒ)"))
            st.markdown("**Notes:**")
            for n in c.get("notes", []):
                st.markdown(f"- {n}")
            st.markdown("**Summary:** " + (c.get("summary") or "(ìš”ì•½ ì—†ìŒ)"))
            st.markdown('<div class="apple-divider"></div>', unsafe_allow_html=True)

        if per_items:
            if primary_button("â¬‡ï¸ Word ë‚´ë³´ë‚´ê¸°(íŒŒì¼ë³„ ë¬¶ìŒ)", key="dl_cornell"):
                docx_bytes = make_cornell_docx_per_files(per_items)
                st.success("âœ… Word íŒŒì¼ ìƒì„± ì™„ë£Œ! ğŸ“„ ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                st.download_button("ğŸ“¥ ë‹¤ìš´ë¡œë“œ", data=docx_bytes, file_name="StudyMind_Cornell_PerFiles.docx",
                                  mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                  use_container_width=True)

# 6) Mood Coaching
elif page == "ğŸ’š ì˜¤ëŠ˜ì˜ ê°ì • ì½”ì¹­":
    with apple_card():
        st.caption("â€¢ ê³µê° â†’ ì›ì¸ê°€ì„¤ â†’ 5ë¶„ ë£¨í‹´ â†’ ë‚´ì¼ì˜ ì‘ì€ ì‹¤ì²œ â†’ ë¦¬ì†ŒìŠ¤")
        mood = st.text_area("ì˜¤ëŠ˜ì˜ ê°ì •/ìƒí™©", placeholder="ì˜ˆ: ë°œí‘œ ì•ë‘ê³  ë¶ˆì•ˆí•´ìš”. ì–´ì œ ì ì„ ê±°ì˜ ëª» ì¤ê³ â€¦", height=140)
        ok = primary_button("ì½”ì¹­ ë°›ê¸°", key="btn_mood")
        
        if ok:
            if mood.strip():
                with st.spinner("AIê°€ ë‹¹ì‹ ì˜ ë§ˆìŒì„ ì½ê³  ìˆìŠµë‹ˆë‹¤..."):
                    out = gpt_with_web_context(PROMPT_MOOD, mood.strip(), temperature=TEMPERATURE)
                    if out: 
                        st.markdown(out)
                        st.success("ë‹¹ì‹ ì€ ì´ë¯¸ ì¢‹ì€ ë°©í–¥ìœ¼ë¡œ ê°€ê³  ìˆì–´ìš”. í•œ ê±¸ìŒì”©, ì˜¤ëŠ˜ë„ ì¶©ë¶„íˆ ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. ğŸŒ¿")
            else:
                st.error("âš ï¸ ì˜¤ëŠ˜ì˜ ê°ì •/ìƒí™©ì„ ì…ë ¥í•œ í›„ ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")